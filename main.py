from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from src.core.settings import settings
from src.core.exceptions import LLMServiceError, InvalidRequestError
from src.api.routes import router

description = """
# FastAPI LangChain AI Chat API

This API provides integration with Ollama's Gemma model for chat completions.

## Features

* ğŸ’¬ Chat completion with regular response
* ğŸŒŠ Streaming chat completion
* ğŸ¯ Custom system prompts support
"""

app = FastAPI(
    title=settings.PROJECT_NAME,
    description=description,
    version="1.0.0",
    # docsì™€ ê´€ë ¨ëœ URLì€ API_V1_STR ì—†ì´ ë£¨íŠ¸ì— ë°°ì¹˜
    openapi_url="/openapi.json",
    docs_url="/docs",
    redoc_url="/redoc",
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(LLMServiceError)
async def llm_service_error_handler(request: Request, exc: LLMServiceError):
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

@app.exception_handler(InvalidRequestError)
async def invalid_request_error_handler(request: Request, exc: InvalidRequestError):
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# API ë¼ìš°í„° ë“±ë¡ - API ì—”ë“œí¬ì¸íŠ¸ë§Œ API_V1_STR í”„ë¦¬í”½ìŠ¤ ì‚¬ìš©
app.include_router(
    router,
    prefix=settings.API_V1_STR,
    tags=["chat"]
)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=settings.HOST, port=settings.PORT)
