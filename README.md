# FastAPI LangChain AI Chat API

Ollamaì˜ Gemma ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” FastAPI ê¸°ë°˜ì˜ ì±„íŒ… API ì„œë²„ì…ë‹ˆë‹¤. ì¼ë°˜ ì‘ë‹µê³¼ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

## ê¸°ëŠ¥

- ğŸ’¬ Chat completion with regular response
- ğŸŒŠ Streaming chat completion (SSE)
- ğŸ¯ Custom system prompts support
- ğŸ”„ UTF-8 ì¸ì½”ë”© ì§€ì›
- ğŸ“ OpenAPI ë¬¸ì„œ (Swagger UI)

## ìš”êµ¬ì‚¬í•­

- Python 3.11 ì´ìƒ
- Ollama (gemma ëª¨ë¸ ì„¤ì¹˜ í•„ìš”)

## ì„¤ì¹˜ ë°©ë²•

1. ì €ì¥ì†Œ í´ë¡ 
```bash
git clone <repository-url>
cd fastapi_langchain
```

2. ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# or
.venv\Scripts\activate  # Windows
```

3. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

4. í™˜ê²½ ì„¤ì •
`.env.example`ì„ `.env`ë¡œ ë³µì‚¬í•˜ê³  í•„ìš”í•œ ì„¤ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤:
```bash
cp .env.example .env
```

## ì‹¤í–‰ ë°©ë²•

```bash
python main.py
```

ì„œë²„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `http://localhost:8000`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

## API ì—”ë“œí¬ì¸íŠ¸

### ì±„íŒ… API
- URL: `/api/v1/chat`
- Method: `POST`
- Content-Type: `application/json`

#### ìš”ì²­ í˜•ì‹
```json
{
    "message": "ì§ˆë¬¸ ë‚´ìš©",
    "system_prompt": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)",
    "stream": false
}
```

#### ì¼ë°˜ ì‘ë‹µ ì˜ˆì‹œ
```json
{
    "response": "ëª¨ë¸ì˜ ì‘ë‹µ ë‚´ìš©"
}
```

#### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‚¬ìš©ë²•

ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì‚¬ìš©í•˜ë ¤ë©´ `stream: true`ë¡œ ì„¤ì •í•˜ê³  Server-Sent Events(SSE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```javascript
const eventSource = new EventSource('/api/v1/chat');
eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.done) {
        eventSource.close();
    } else {
        console.log(data.text);
    }
};
```

## API ë¬¸ì„œ

- Swagger UI: `/docs`
- ReDoc: `/redoc`
- OpenAPI JSON: `/openapi.json`

## ìƒíƒœ í™•ì¸

- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸: `/health`

## ì„¤ì • ì˜µì…˜ (.env)

```ini
# API Configuration
API_V1_STR=/api/v1
PROJECT_NAME="FastAPI LangChain Project"

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=gemma3

# Server Configuration
HOST=0.0.0.0
PORT=8000
```

## ì—ëŸ¬ ì²˜ë¦¬

APIëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—ëŸ¬ ì‘ë‹µì„ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- 400: ì˜ëª»ëœ ìš”ì²­ (ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì¸ì½”ë”© ì˜¤ë¥˜)
- 503: LLM ì„œë¹„ìŠ¤ ì˜¤ë¥˜ (Ollama ì—°ê²° ì‹¤íŒ¨ ë“±)
- 500: ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜

ê° ì—ëŸ¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤:
```json
{
    "error": "ì—ëŸ¬ ë©”ì‹œì§€"
}
```

## ë¼ì´ì„ ìŠ¤

[ë¼ì´ì„ ìŠ¤ ì •ë³´]